{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all webarchive pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "night_tours_dates_url = \"https://web.archive.org/cdx/search/cdx?url=https://www.nighttours.com/gaypride/&output=json\"\n",
    "\n",
    "# night_tours_dates_resource = urllib.request.urlopen(night_tours_dates_url, timeout=20)\n",
    "# night_tours_dates_content =  night_tours_dates_resource.read().decode(\n",
    "#     night_tours_dates_resource.headers.get_content_charset())\n",
    "# night_tours_dates = json.loads(night_tours_dates_content)\n",
    "night_tours_dates_resource = requests.get(night_tours_dates_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "night_tours_dates_content =  night_tours_dates_resource.content.decode(\n",
    "    night_tours_dates_resource.encoding)\n",
    "night_tours_dates = json.loads(night_tours_dates_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_col = night_tours_dates[0].index('timestamp')\n",
    "night_tours_dates = [snapshot[timestamp_col] for snapshot in night_tours_dates[1:]]\n",
    "night_tours_dates = sorted(list(set(night_tours_dates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 268/268 [2:02:55<00:00, 27.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# for ts in [night_tours_dates[0], \n",
    "#            night_tours_dates[len(night_tours_dates)//3], \n",
    "#            night_tours_dates[2 * len(night_tours_dates)//3],\n",
    "#            night_tours_dates[-1]]:\n",
    "headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    'Accept-language': 'he-IL,he;q=0.9,en-US;q=0.8,en;q=0.7,fr;q=0.6',\n",
    "#     'Content-Type': 'application/json',\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',    \n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5,\n",
    "                backoff_factor=0.5,\n",
    "                status_forcelist=[ 500, 502, 503, 504 ],\n",
    "                connect=3)\n",
    "\n",
    "# retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "for ts in tqdm(night_tours_dates[len(soups):]):\n",
    "#     resource = urllib.request.urlopen(f\"https://web.archive.org/web/{ts}/https://www.nighttours.com/gaypride/\")\n",
    "    resource = session.get(f\"https://web.archive.org/web/{ts}/http://www.nighttours.com/gaypride/\",\n",
    "                            timeout=120, headers=headers)\n",
    "#     data =  resource.read().decode(resource.headers.get_content_charset())\n",
    "    \n",
    "    with open(f'gayparades_htmls\\\\{ts}_{resource.encoding}_{resource.apparent_encoding}.html', \"wb\") as f:\n",
    "        f.write(resource.content)\n",
    "\n",
    "    time.sleep(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = []\n",
    "for path in sorted(os.listdir('gayparades_htmls')):\n",
    "    html = os.path.join('gayparades_htmls', path)\n",
    "    with open(html, 'rb') as f:\n",
    "        resource = f.read()\n",
    "    \n",
    "    ts, encoding, apparent_encoding = path.split('.')[0].split('_')\n",
    "    \n",
    "    try:\n",
    "        data = resource.decode(encoding)\n",
    "    except UnicodeDecodeError:\n",
    "        data = resource.decode(apparent_encoding)\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    soups.append((soup, ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 20061026221710: no pridelist\n",
      "Skipping 20061208225851: no pridelist\n",
      "Skipping 20061213175746: no pridelist\n",
      "Skipping 20061218102202: no pridelist\n",
      "Skipping 20061223013001: no pridelist\n",
      "Skipping 20061231095217: no pridelist\n",
      "Skipping 20070105015532: no pridelist\n",
      "Skipping 20070107032131: no pridelist\n",
      "Skipping 20070109205651: no pridelist\n",
      "Skipping 20070114195410: no pridelist\n",
      "Skipping 20070119213526: no pridelist\n",
      "Skipping 20070125091636: no pridelist\n",
      "Skipping 20070206002150: no pridelist\n",
      "Skipping 20070307091905: no pridelist\n",
      "Skipping 20070316043702: no pridelist\n",
      "Skipping 20070324050528: no pridelist\n",
      "Skipping 20070509150816: no pridelist\n",
      "Skipping 20070518172714: no pridelist\n",
      "Skipping 20070625174903: no pridelist\n",
      "Skipping 20070627061632: no pridelist\n",
      "Skipping 20070717074437: no pridelist\n",
      "Skipping 20070810140931: no pridelist\n",
      "Skipping 20070823150350: no pridelist\n",
      "Skipping 20070925160559: no pridelist\n",
      "Skipping 20071008044920: no pridelist\n",
      "Skipping 20071011072533: no pridelist\n",
      "Skipping 20071018040432: no pridelist\n",
      "Skipping 20071117111838: no pridelist\n",
      "Skipping 20071217191524: no pridelist\n",
      "Skipping 20080117051103: no pridelist\n",
      "Skipping 20080217232117: no pridelist\n",
      "Skipping 20080319103315: no pridelist\n",
      "Skipping 20080419012735: no pridelist\n",
      "Skipping 20080526135008: no pridelist\n",
      "Skipping 20080625230919: no pridelist\n",
      "Skipping 20080703040739: no pridelist\n",
      "Skipping 20080709020547: no pridelist\n",
      "Skipping 20080724104742: no pridelist\n",
      "Skipping 20080913071856: no pridelist\n",
      "Skipping 20080930032343: no pridelist\n",
      "Skipping 20081102021536: no pridelist\n",
      "Skipping 20081220152307: no pridelist\n",
      "Skipping 20090121173320: no pridelist\n",
      "Skipping 20090219140144: no pridelist\n",
      "Skipping 20090221180808: no pridelist\n",
      "Skipping 20090222192240: no pridelist\n",
      "Skipping 20090421070047: no pridelist\n",
      "Skipping 20090820180634: no pridelist\n",
      "Skipping 20100114173435: no pridelist\n",
      "Skipping 20100209005320: no pridelist\n",
      "Skipping 20100424005357: no pridelist\n",
      "Skipping 20100429143621: no pridelist\n",
      "Skipping 20101009205817: no pridelist\n",
      "Skipping 20101023064719: no pridelist\n",
      "Skipping 20101129021808: no pridelist\n",
      "Skipping 20101221204723: no pridelist\n",
      "Skipping 20101222151431: no pridelist\n",
      "Skipping 20110221072625: no pridelist\n",
      "Skipping 20110423050351: no pridelist\n",
      "Skipping 20110520094401: no pridelist\n",
      "Skipping 20110622195934: no pridelist\n",
      "Skipping 20110810142048: no pridelist\n",
      "Skipping 20110822223024: no pridelist\n",
      "Skipping 20110925165843: no pridelist\n",
      "Skipping 20111007102510: no pridelist\n",
      "Skipping 20111011122249: no pridelist\n",
      "Skipping 20111016162129: no pridelist\n",
      "Skipping 20111110152134: no pridelist\n",
      "Skipping 20111201092600: no pridelist\n",
      "Skipping 20111222012504: no pridelist\n",
      "Skipping 20120101020727: no pridelist\n",
      "Skipping 20120118221731: no pridelist\n",
      "Skipping 20120214203114: no pridelist\n",
      "Skipping 20120228150415: no pridelist\n",
      "Skipping 20120314160457: no pridelist\n",
      "Skipping 20120405022352: no pridelist\n",
      "Skipping 20120421031942: no pridelist\n",
      "Skipping 20120501212128: no pridelist\n",
      "Skipping 20120502004836: no pridelist\n",
      "Skipping 20120508112932: no pridelist\n",
      "Skipping 20120514081857: no pridelist\n",
      "Skipping 20120615101504: no pridelist\n",
      "Skipping 20120616003919: no pridelist\n",
      "Skipping 20120625184950: no pridelist\n",
      "Skipping 20120804155106: no pridelist\n",
      "Skipping 20120819214633: no pridelist\n",
      "Skipping 20120820063226: no pridelist\n",
      "Skipping 20120825082342: no pridelist\n",
      "Skipping 20120910164826: no pridelist\n",
      "Skipping 20120915052450: no pridelist\n",
      "Skipping 20121026150829: no pridelist\n",
      "Skipping 20180722004459: no event div\n",
      "Skipping 20180723062207: no event div\n",
      "Skipping 20180807192211: no event div\n",
      "Skipping 20180824214649: no event div\n",
      "Skipping 20180826020351: no event div\n",
      "Skipping 20180921111901: no event div\n",
      "Skipping 20180926141106: no event div\n",
      "Skipping 20180926171433: no event div\n",
      "Skipping 20181008233855: no event div\n",
      "Skipping 20181008233856: no event div\n",
      "Skipping 20181029031554: no event div\n",
      "Skipping 20181206143759: no event div\n",
      "Skipping 20181208102934: no event div\n",
      "Skipping 20190107094312: no event div\n",
      "Skipping 20190209114110: no event div\n",
      "Skipping 20190210141421: no event div\n",
      "Skipping 20190215183258: no event div\n",
      "Skipping 20190309112703: no event div\n",
      "Skipping 20190319050729: no event div\n",
      "Skipping 20190324102639: no event div\n",
      "Skipping 20190412135919: no event div\n",
      "Skipping 20190413165105: no event div\n",
      "Skipping 20190423055216: no event div\n",
      "Skipping 20190423232601: no event div\n",
      "Skipping 20190424100409: no event div\n",
      "Skipping 20190512110942: no event div\n",
      "Skipping 20190515045849: no event div\n",
      "Skipping 20190528084140: no event div\n",
      "Skipping 20190613072623: no event div\n",
      "Skipping 20190628114805: no event div\n",
      "Skipping 20190811054711: no event div\n",
      "Skipping 20190814102240: no event div\n",
      "Skipping 20190911024932: no event div\n",
      "Skipping 20200225054201: no event div\n",
      "Skipping 20200225054213: no event div\n",
      "Skipping 20200316004457: no event div\n",
      "Skipping 20200511105542: no event div\n",
      "Skipping 20200815065534: no event div\n",
      "Skipping 20200825165612: no event div\n",
      "Skipping 20200825165614: no event div\n",
      "Skipping 20201020203916: no event div\n",
      "Skipping 20201020203919: no event div\n",
      "Skipping 20201020204014: no event div\n",
      "Skipping 20201111235103: no event div\n",
      "Skipping 20210223103343: no event div\n",
      "Skipping 20210223181023: no event div\n",
      "Skipping 20210322210415: no event div\n",
      "Skipping 20210322210416: no event div\n",
      "Skipping 20210330121843: no event div\n",
      "Skipping 20210330123145: no event div\n",
      "Skipping 20210422021109: no event div\n",
      "Skipping 20210918154342: no event div\n",
      "Skipping 20220118142544: no event div\n",
      "Skipping 20220201084449: no event div\n",
      "Skipping 20220303172758: no event div\n",
      "Skipping 20220423111305: no event div\n",
      "Skipping 20220519193046: no event div\n",
      "Skipping 20220626025938: no event div\n",
      "Skipping 20220714214246: no event div\n",
      "Skipping 20220810090812: no event div\n",
      "Skipping 20220819070421: no event div\n",
      "Skipping 20220901034345: no event div\n",
      "Skipping 20221119165248: no event div\n",
      "Skipping 20221220120224: no event div\n",
      "Skipping 20230110021932: no event div\n",
      "Processed 112/268 htmls.\n",
      " So far 4932 dates.\n"
     ]
    }
   ],
   "source": [
    "skipped_soups = []\n",
    "dates = []\n",
    "for s, ts in soups:\n",
    "    pridelist = s.find(\"div\", {\"id\": \"pridelist\"})\n",
    "    if pridelist is None:\n",
    "        print(f'Skipping {ts}: no pridelist')\n",
    "        skipped_soups.append((s,ts))\n",
    "        continue\n",
    "        \n",
    "    events = pridelist.find_all(\"div\", {\"itemtype\": \"http://schema.org/Event\"})\n",
    "    if not len(events):\n",
    "        print(f'Skipping {ts}: no event div')\n",
    "        skipped_soups.append((s,ts))\n",
    "        continue\n",
    "\n",
    "    for e in events:\n",
    "        if 'cancelled' in e.get_text():\n",
    "                    continue\n",
    "                \n",
    "        if e.find('span', {'class': 'flag_us'}) is not None:\n",
    "            start_date = e.find('time', {'itemprop': 'startDate'}).attrs['datetime']\n",
    "            end_date = e.find('time', {'itemprop': 'endDate'})\n",
    "            if end_date is None:\n",
    "                end_date = start_date\n",
    "            else:\n",
    "                end_date = end_date.attrs['datetime']\n",
    "            start_date = datetime.date.fromisoformat(start_date)\n",
    "            end_date = datetime.date.fromisoformat(end_date)\n",
    "            location = e.find('span', {'itemprop': 'addressLocality'}).get_text()\n",
    "            description = e.find('span', {'itemprop': 'name'}).get_text()\n",
    "            dates.append({\n",
    "                'location': location,\n",
    "                'description': description,\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date,\n",
    "                'flagged': True, 'ts': ts\n",
    "            })\n",
    "\n",
    "print(f'Processed {len(soups) - len(skipped_soups)}/{len(soups)} htmls.\\n So far {len(dates)} dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 20070316043702: no pridelist, no maincontent\n",
      "Skipping 20070324050528: no pridelist, no maincontent\n",
      "Skipping 20070509150816: no pridelist, no maincontent\n",
      "Skipping 20070518172714: no pridelist, no maincontent\n",
      "Skipping 20070625174903: no pridelist, no maincontent\n",
      "Skipping 20070627061632: no pridelist, no maincontent\n",
      "Skipping 20070717074437: no pridelist, no maincontent\n",
      "Skipping 20070810140931: no pridelist, no maincontent\n",
      "Skipping 20070823150350: no pridelist, no maincontent\n",
      "Skipping 20070925160559: no pridelist, no maincontent\n",
      "Skipping 20071008044920: no pridelist, no maincontent\n",
      "Skipping 20071011072533: no pridelist, no maincontent\n",
      "Skipping 20071018040432: no pridelist, no maincontent\n",
      "Skipping 20071117111838: no pridelist, no maincontent\n",
      "Skipping 20071217191524: no pridelist, no maincontent\n",
      "Skipping 20080117051103: no pridelist, no maincontent\n",
      "Skipping 20080217232117: no pridelist, no maincontent\n",
      "Skipping 20080319103315: no pridelist, no maincontent\n",
      "Skipping 20080419012735: no pridelist, no maincontent\n",
      "Skipping 20080526135008: no pridelist, no maincontent\n",
      "Skipping 20080625230919: no pridelist, no maincontent\n",
      "Skipping 20080703040739: no pridelist, no maincontent\n",
      "Skipping 20080709020547: no pridelist, no maincontent\n",
      "Skipping 20080724104742: no pridelist, no maincontent\n",
      "Skipping 20080913071856: no pridelist, no maincontent\n",
      "Skipping 20080930032343: no pridelist, no maincontent\n",
      "Skipping 20081102021536: no pridelist, no maincontent\n",
      "Skipping 20081220152307: no pridelist, no maincontent\n",
      "Skipping 20090121173320: no pridelist, no maincontent\n",
      "Skipping 20090219140144: no pridelist, no maincontent\n",
      "Skipping 20090221180808: no pridelist, no maincontent\n",
      "Skipping 20090222192240: no pridelist, no maincontent\n",
      "Skipping 20090421070047: no pridelist, no maincontent\n",
      "Skipping 20090820180634: no pridelist, no maincontent\n",
      "Skipping 20100114173435: no pridelist, no maincontent\n",
      "Skipping 20100209005320: no pridelist, no maincontent\n",
      "Skipping 20100424005357: no pridelist, no maincontent\n",
      "Skipping 20100429143621: no pridelist, no maincontent\n",
      "Skipping 20101009205817: no pridelist, no maincontent\n",
      "Skipping 20101023064719: no pridelist, no maincontent\n",
      "Skipping 20101129021808: no pridelist, no maincontent\n",
      "Skipping 20101221204723: no pridelist, no maincontent\n",
      "Skipping 20101222151431: no pridelist, no maincontent\n",
      "Skipping 20110221072625: no pridelist, no maincontent\n",
      "Skipping 20110423050351: no pridelist, no maincontent\n",
      "Skipping 20110520094401: no pridelist, no maincontent\n",
      "Skipping 20110622195934: no pridelist, no maincontent\n",
      "Skipping 20110810142048: no pridelist, no maincontent\n",
      "Skipping 20110822223024: no pridelist, no maincontent\n",
      "Skipping 20110925165843: no pridelist, no maincontent\n",
      "Skipping 20111007102510: no pridelist, no maincontent\n",
      "Skipping 20111011122249: no pridelist, no maincontent\n",
      "Skipping 20111016162129: no pridelist, no maincontent\n",
      "Skipping 20111110152134: no pridelist, no maincontent\n",
      "Skipping 20111201092600: no pridelist, no maincontent\n",
      "Skipping 20111222012504: no pridelist, no maincontent\n",
      "Skipping 20120101020727: no pridelist, no maincontent\n",
      "Skipping 20120118221731: no pridelist, no maincontent\n",
      "Skipping 20120214203114: no pridelist, no maincontent\n",
      "Skipping 20120228150415: no pridelist, no maincontent\n",
      "Skipping 20120314160457: no pridelist, no maincontent\n",
      "Skipping 20120405022352: no pridelist, no maincontent\n",
      "Skipping 20120421031942: no pridelist, no maincontent\n",
      "Skipping 20120501212128: no pridelist, no maincontent\n",
      "Skipping 20120502004836: no pridelist, no maincontent\n",
      "Skipping 20120508112932: no pridelist, no maincontent\n",
      "Skipping 20120514081857: no pridelist, no maincontent\n",
      "Skipping 20120615101504: no pridelist, no maincontent\n",
      "Skipping 20120616003919: no pridelist, no maincontent\n",
      "Skipping 20120625184950: no pridelist, no maincontent\n",
      "Skipping 20120804155106: no pridelist, no maincontent\n",
      "Skipping 20120819214633: no pridelist, no maincontent\n",
      "Skipping 20120820063226: no pridelist, no maincontent\n",
      "Skipping 20120825082342: no pridelist, no maincontent\n",
      "Skipping 20120910164826: no pridelist, no maincontent\n",
      "Skipping 20120915052450: no pridelist, no maincontent\n",
      "Skipping 20121026150829: no pridelist, no maincontent\n",
      "Skipping 20190107094312: no event li\n",
      "Skipping 20190209114110: no event li\n",
      "Skipping 20190210141421: no event li\n",
      "Skipping 20190215183258: no event li\n",
      "Skipping 20190309112703: no event li\n",
      "Skipping 20190319050729: no event li\n",
      "Skipping 20190324102639: no event li\n",
      "Skipping 20190412135919: no event li\n",
      "Skipping 20190413165105: no event li\n",
      "Skipping 20190423055216: no event li\n",
      "Skipping 20190423232601: no event li\n",
      "Skipping 20190424100409: no event li\n",
      "Skipping 20190512110942: no event li\n",
      "Skipping 20190515045849: no event li\n",
      "Skipping 20190528084140: no event li\n",
      "Skipping 20190613072623: no event li\n",
      "Skipping 20190628114805: no event li\n",
      "Skipping 20190811054711: no event li\n",
      "Skipping 20190814102240: no event li\n",
      "Skipping 20190911024932: no event li\n",
      "Skipping 20200225054201: no event li\n",
      "Skipping 20200225054213: no event li\n",
      "Skipping 20200316004457: no event li\n",
      "Skipping 20200511105542: no event li\n",
      "Skipping 20200815065534: no event li\n",
      "Skipping 20200825165612: no event li\n",
      "Skipping 20200825165614: no event li\n",
      "Skipping 20201020203916: no event li\n",
      "Skipping 20201020203919: no event li\n",
      "Skipping 20201020204014: no event li\n",
      "Skipping 20201111235103: no event li\n",
      "Skipping 20210223103343: no event li\n",
      "Skipping 20210223181023: no event li\n",
      "Skipping 20210322210415: no event li\n",
      "Skipping 20210322210416: no event li\n",
      "Skipping 20210330121843: no event li\n",
      "Skipping 20210330123145: no event li\n",
      "Skipping 20210422021109: no event li\n",
      "Skipping 20210918154342: no event li\n",
      "Skipping 20220118142544: no event li\n",
      "Skipping 20220201084449: no event li\n",
      "Skipping 20220303172758: no event li\n",
      "Skipping 20220423111305: no event li\n",
      "Skipping 20220519193046: no event li\n",
      "Skipping 20220626025938: no event li\n",
      "Skipping 20220714214246: no event li\n",
      "Skipping 20220810090812: no event li\n",
      "Skipping 20220819070421: no event li\n",
      "Skipping 20220901034345: no event li\n",
      "Skipping 20221119165248: no event li\n",
      "Skipping 20221220120224: no event li\n",
      "Skipping 20230110021932: no event li\n",
      "Processed 139/268 htmls.\n",
      " So far 5829 dates.\n"
     ]
    }
   ],
   "source": [
    "MONTHS_NAMES = {\n",
    "    'jan': 1,\n",
    "    'gen': 1,\n",
    "    'feb': 2,\n",
    "    'fév': 2,\n",
    "    'mar': 3,\n",
    "    'apr': 4,\n",
    "    'avr': 4,\n",
    "    'may': 5,\n",
    "    'mag': 5,\n",
    "    'mai': 5,\n",
    "    'jun': 6,\n",
    "    'giu': 6,\n",
    "    'jul': 7,\n",
    "    'lug': 7,\n",
    "    'aug': 8,\n",
    "    'ago': 8,\n",
    "    'aou': 8,\n",
    "    'sep': 9,\n",
    "    'set': 9,\n",
    "    'oct': 10,\n",
    "    'ott': 10,\n",
    "    'nov': 11,\n",
    "    'dec': 12,\n",
    "    'déc': 12,\n",
    "    'dic': 12,\n",
    "}\n",
    "\n",
    "WEEKDAYS = '(Sun|Mon|Tue|Wed|Thu|Fri|Sat)'\n",
    "MONTHS = '(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|' \\\n",
    "          'gen|feb|mar|apr|mag|giu|lug|ago|set|ott|nov|dic|' \\\n",
    "          'Fév|Avr|Mai|Aou|Déc)'\n",
    "\n",
    "skipped_soups2 = []\n",
    "for s, ts in skipped_soups:\n",
    "    pridelist = s.find(\"div\", {\"id\": \"pridelist\"})\n",
    "    if pridelist is None:\n",
    "        maincontent = s.find(\"div\", {\"id\": \"maincontent\"})\n",
    "        if maincontent is None:\n",
    "            print(f'Skipping {ts}: no pridelist, no maincontent')\n",
    "            skipped_soups2.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        maincontent_txt = maincontent.get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' ')       \n",
    "        matches = re.findall(f'\\s*(.+?)\\n'\n",
    "                             f'((\\d+)\\s+?[-–]\\s+?(\\d+)\\s+?{MONTHS}|'\n",
    "                             f'(\\d+)\\s+?{MONTHS}\\s+?[-–]\\s+?(\\d+)\\s+?{MONTHS}|'\n",
    "                             f'(\\d+)\\s+?{MONTHS})\\s*?(\\d\\d\\d\\d)?\\n(.+?)\\n', \n",
    "                             maincontent_txt)\n",
    "        if not len(matches):\n",
    "            print(f'Skipping {ts}: regex matches')\n",
    "            skipped_soups2.append((s,ts))\n",
    "            continue\n",
    "        \n",
    "        dates += [{'location': m[0], 'description': m[-1],\n",
    "                   'start_date': datetime.date(int(m[-2] if len(m[-2]) else ts[:4]),\n",
    "                                               MONTHS_NAMES[(m[4]+m[6]+m[10]).lower()],\n",
    "                                               int(m[2]+m[5]+m[9])),\n",
    "                   'end_date': datetime.date(int(m[-2] if len(m[-2]) else ts[:4]),\n",
    "                                             MONTHS_NAMES[(m[4]+m[8]+m[10]).lower()],\n",
    "                                             int(m[3]+m[7]+m[9])),\n",
    "                  'flagged': False, 'ts': ts} for m in matches]\n",
    "    else:\n",
    "        # pridelist but no events\n",
    "        events = pridelist.find_all(\"li\", {\"itemtype\": \"http://schema.org/Event\"})\n",
    "        if not len(events):\n",
    "            print(f'Skipping {ts}: no event li')\n",
    "            skipped_soups2.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        for e in events:\n",
    "            if 'cancelled' in e.get_text():\n",
    "                    continue\n",
    "            \n",
    "            if e.find('span', {'class': 'flag_us'}) is not None:\n",
    "                start_date = e.find('time', {'itemprop': 'startDate'}).attrs['datetime']\n",
    "                end_date = e.find('time', {'itemprop': 'endDate'})\n",
    "                if end_date is None:\n",
    "                    end_date = start_date\n",
    "                else:\n",
    "                    end_date = end_date.attrs['datetime']\n",
    "                start_date = datetime.date.fromisoformat(start_date)\n",
    "                end_date = datetime.date.fromisoformat(end_date)\n",
    "                location = e.find('span', {'itemprop': 'addressLocality'}).get_text()\n",
    "                description = e.find('span', {'itemprop': 'name'}).get_text()\n",
    "                dates.append({\n",
    "                    'location': location,\n",
    "                    'description': description,\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'flagged': True, 'ts': ts\n",
    "                })\n",
    "\n",
    "print(f'Processed {len(soups) - len(skipped_soups2)}/{len(soups)} htmls.\\n So far {len(dates)} dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 20081220152307: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20090121173320: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20090219140144: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20090221180808: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20090222192240: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20090421070047: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20090820180634: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20100114173435: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20100209005320: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20100424005357: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20100429143621: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20101009205817: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20101023064719: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20101129021808: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20101221204723: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20101222151431: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20110221072625: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20110423050351: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20110520094401: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20110622195934: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20110810142048: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20110822223024: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20110925165843: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20111007102510: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20111011122249: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20111016162129: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20111110152134: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20111201092600: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20111222012504: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120101020727: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120118221731: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120214203114: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120228150415: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120314160457: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120405022352: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120421031942: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120501212128: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120502004836: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120508112932: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120514081857: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120615101504: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120616003919: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120625184950: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120804155106: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120819214633: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120820063226: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120825082342: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120910164826: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20120915052450: no pridelist, no maincontent, no leftcolumn\n",
      "Skipping 20121026150829: no pridelist, no maincontent, no leftcolumn\n",
      "Processed 218/268 htmls.\n",
      " So far 7765 dates.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "skipped_soups3 = []\n",
    "for s, ts in skipped_soups2:\n",
    "    pridelist = s.find(\"div\", {\"id\": \"pridelist\"})\n",
    "    if pridelist is None:\n",
    "        maincontent = s.find(\"div\", {\"id\": \"maincontent\"})\n",
    "        if maincontent is None:\n",
    "            leftcolumn = s.find(\"div\", {\"id\": \"leftcolumn\"})\n",
    "            if leftcolumn is None:\n",
    "                print(f'Skipping {ts}: no pridelist, no maincontent, no leftcolumn')\n",
    "                skipped_soups3.append((s,ts))\n",
    "                continue\n",
    "            \n",
    "            events = leftcolumn.find_all('div', {'class': 'rowoff'})\n",
    "            if not len(events):\n",
    "                print(f'Skipping {ts}: no rowoff div')\n",
    "                skipped_soups3.append((s,ts))\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            if events[0].find('div', {'class': 'column_desc'}) is None or events[0].find_all('div', {'class': 'column_dateplace'}) is None:\n",
    "                    print(f'Skipping {ts}: no column_desc or date_place div')\n",
    "                    skipped_soups3.append((s,ts))\n",
    "                    continue\n",
    "            \n",
    "            for e in events:\n",
    "                if 'cancelled' in e.get_text():\n",
    "                    continue\n",
    "                \n",
    "                # try to find flag\n",
    "                if e.find('img') is not None:\n",
    "                    if 'us' not in e.find('img').attrs['alt'].lower():\n",
    "                        continue\n",
    "                \n",
    "                desc = e.find('div', {'class': 'column_desc'}).get_text()\n",
    "                loc, date = e.find_all('div', {'class': 'column_dateplace'})\n",
    "                \n",
    "                m = re.findall('((\\d+)\\s+?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)|'\n",
    "                           '(\\d+)\\s+?[-–]\\s+?(\\d+) (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)|'\n",
    "                           '(\\d+)\\s+?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+?[-–]\\s+?'\n",
    "                           '(\\d+)\\s+?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))(\\s+?(20\\d\\d))?',\n",
    "                               date.get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' '))[0]\n",
    "                \n",
    "                dates.append({'location': loc.get_text(),\n",
    "                              'description': desc,\n",
    "                              'start_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                  MONTHS_NAMES[(m[2]+m[5]+m[7]).lower()], int(m[1]+m[3]+m[6])),\n",
    "                              'end_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                 MONTHS_NAMES[(m[2]+m[5]+m[9]).lower()], int(m[1]+m[4]+m[8])),\n",
    "                              'flagged': False, 'ts': ts\n",
    "                             })\n",
    "        else:\n",
    "            maincontent_txt = maincontent.get_text()\n",
    "            matches = re.findall('\\s*(.+?)\\n((\\d+)\\s+?'\n",
    "                                 '(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)|'\n",
    "                                 '(\\d+)\\s+?[-–]\\s+?(\\d+) (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)|'\n",
    "                                 '(\\d+)\\s+?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+?'\n",
    "                                 '[-–]\\s+?(\\d+) (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))(\\s+?'\n",
    "                                 '(20\\d\\d))?\\n(.+?)\\n', maincontent_txt)\n",
    "            if not len(matches):\n",
    "                print(f'Skipping {ts}: regex matches2')\n",
    "                skipped_soups3.append((s,ts))\n",
    "                continue\n",
    "        \n",
    "            dates += [{'location': m[0],\n",
    "                       'description': m[-1],\n",
    "                       'start_date': datetime.date(int(m[-2] if len(m[-2]) else ts[:4]), \n",
    "                                                   MONTHS_NAMES[(m[3]+m[6]+m[8]).lower()], int(m[2]+m[4]+m[7])),\n",
    "                       'end_date': datetime.date(int(m[-2] if len(m[-2]) else ts[:4]), \n",
    "                                                 MONTHS_NAMES[(m[3]+m[6]+m[10]).lower()], int(m[2]+m[5]+m[9])),\n",
    "                       'flagged': False, 'ts': ts\n",
    "                       } for m in matches]\n",
    "    else:\n",
    "        # pridelist but no events\n",
    "        events = pridelist.find_all(\"li\")\n",
    "        if not len(events):\n",
    "            print(f'Skipping {ts}: no event li at all')\n",
    "            skipped_soups3.append((s,ts))\n",
    "            continue\n",
    "        \n",
    "        if pridelist.find('span', {'class': 'flag_us'}) is None and pridelist.find('img', {'alt': 'flag USA'}) is None \\\n",
    "            and pridelist.find('span', {'class': 'flag-us'}) is None:\n",
    "            gaegaeg\n",
    "            print(f'Skipping {ts}: no flags?')\n",
    "            skipped_soups3.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            for e in events:\n",
    "                if 'cancelled' in e.get_text():\n",
    "                    continue\n",
    "                \n",
    "                if e.find('span', {'class': 'flag_us'}) is not None or e.find('img', {'alt': 'flag USA'}) is not None or \\\n",
    "                    e.find('span', {'class': 'flag-us'}) is not None:\n",
    "                    date = e.find('span', {'class': 'cel-date'})\n",
    "                    if date is not None:\n",
    "                        m = re.findall(f'((\\d+)\\s+?[-–]\\s+?(\\d+)\\s+?{MONTHS}|'\n",
    "                                       f'(\\d+)\\s+?{MONTHS}\\s+?[-–]\\s+?(\\d+)\\s+?{MONTHS}|'\n",
    "                                       f'(\\d+)\\s+?{MONTHS})(\\s+?(20\\d\\d))?',\n",
    "                                       date.get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' '))\n",
    "                        \n",
    "                        if len(m):\n",
    "                            m = m[0]\n",
    "                        else:\n",
    "                            raise NotImplementedError(\"matches date\")\n",
    "                        \n",
    "                        city = e.find('span', {'class': 'cel-city-event'})\n",
    "                        if city is None:\n",
    "                            city = e.find('span', {'class': 'cel-info-after-arrow'})\n",
    "\n",
    "                        dates.append({'location': city.contents[-1].get_text(),\n",
    "                          'description': (str(city.contents[0]) if type(city.contents[0]) == NavigableString else\n",
    "                                          city.contents[0].get_text()),\n",
    "                          'start_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                      MONTHS_NAMES[(m[3]+m[5]+m[9]).lower()],\n",
    "                                                      int(m[1]+m[4]+m[8])),\n",
    "                          'end_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                    MONTHS_NAMES[(m[3]+m[7]+m[9]).lower()],\n",
    "                                                    int(m[2]+m[6]+m[8])),\n",
    "                          'flagged': True, 'ts': ts\n",
    "                         })\n",
    "                    else: \n",
    "                        desc = e.find('span', {'class': 'cel-info'}).contents[0].get_text()\n",
    "                        citydate = e.find('span', {'class': 'cel-city'})\n",
    "\n",
    "                        m = re.findall(f'\\s*(.+?)\\s+?–\\s+?('\n",
    "                                       f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                                       f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                                       f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS})(\\s+(20\\d\\d))?', \n",
    "                                       citydate.get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' '))\n",
    "                        \n",
    "                        if len(m):\n",
    "                            m = m[0]\n",
    "                        else:\n",
    "                            if 'TBA' in citydate.get_text():\n",
    "                                continue\n",
    "                            else:\n",
    "                                afefeaf\n",
    "                                raise NotImplementedError(\"matches\")\n",
    "\n",
    "                        dates.append({'location': m[0],\n",
    "                                      'description': desc,\n",
    "                                      'start_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                                  MONTHS_NAMES[(m[8]+m[12]+m[20]).lower()], \n",
    "                                                                  int(m[4]+m[11]+m[19])),\n",
    "                                      'end_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                                MONTHS_NAMES[(m[8]+m[16]+m[20]).lower()], \n",
    "                                                                int(m[7]+m[15]+m[19])),\n",
    "                                      'flagged': True, 'ts': ts\n",
    "                                     })\n",
    "        except NotImplementedError as ee:\n",
    "            print(f'Skipping {ts}: matches{ee}')\n",
    "            skipped_soups3.append((s,ts))\n",
    "            continue\n",
    "                         \n",
    "\n",
    "\n",
    "print(f'Processed {len(soups) - len(skipped_soups3)}/{len(soups)} htmls.\\n So far {len(dates)} dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 20081220152307: no pridelist, no list_full_events\n",
      "Skipping 20090121173320: no pridelist, no list_full_events\n",
      "Skipping 20090219140144: no pridelist, no list_full_events\n",
      "Skipping 20090221180808: no pridelist, no list_full_events\n",
      "Skipping 20090222192240: no pridelist, no list_full_events\n",
      "Skipping 20090421070047: no pridelist, no list_full_events\n",
      "Skipping 20090820180634: no pridelist, no list_full_events\n",
      "Skipping 20100114173435: no events\n",
      "Skipping 20100209005320: no events\n",
      "Skipping 20100424005357: no events\n",
      "Skipping 20100429143621: no events\n",
      "Processed 257/268 htmls.\n",
      " So far 9378 dates.\n"
     ]
    }
   ],
   "source": [
    "skipped_soups4 = []\n",
    "for s, ts in skipped_soups3:\n",
    "    list_full_events = s.find(\"div\", {\"class\": \"list_full_events\"})\n",
    "    if list_full_events is None:\n",
    "        print(f'Skipping {ts}: no pridelist, no list_full_events')\n",
    "        skipped_soups4.append((s,ts))\n",
    "        continue\n",
    "\n",
    "    events = list_full_events.find_all(\"div\", {\"itemtype\": \"http://schema.org/Event\"})\n",
    "    if not len(events):\n",
    "        content_basic468 = s.find(\"div\", {\"class\": \"content_basic468\"})\n",
    "        if content_basic468 is None:\n",
    "            print(f'Skipping {ts}: no events')\n",
    "            skipped_soups4.append((s,ts))\n",
    "            continue\n",
    "        \n",
    "        events = list_full_events.find_all(\"a\")\n",
    "        if not len(events):\n",
    "            print(f'Skipping {ts}: basic but no events')\n",
    "            skipped_soups4.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        if list_full_events.find('span', {'class': 'flag_us'}) is None:\n",
    "            print(f'Skipping {ts}: basic but no flags')\n",
    "            skipped_soups4.append((s,ts))\n",
    "            continue\n",
    "        \n",
    "        for e in events:\n",
    "            if e.find('span', {'class': 'flag_us'}) is not None:\n",
    "                loc = e.find('span', {'class': 'cel2'}).get_text()             \n",
    "                desc = e.find('span', {'class': 'cel4'}).get_text()             \n",
    "                date = e.find('span', {'class': 'cel3'}).get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' ')\n",
    "\n",
    "                m = re.findall(f'(({WEEKDAYS}\\s+?)?(\\d+)\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                               f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                               f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS})(\\s+(20\\d\\d))?', date)\n",
    "                \n",
    "                if len(m):\n",
    "                    m = m[0]\n",
    "                else:\n",
    "                    if 'TBA' in citydate.get_text():\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"matches\")\n",
    "\n",
    "                dates.append({'location': loc,\n",
    "                              'description': desc,\n",
    "                              'start_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                          MONTHS_NAMES[(m[7]+m[11]+m[19]).lower()], \n",
    "                                                          int(m[3]+m[10]+m[18])),\n",
    "                                      'end_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                                MONTHS_NAMES[(m[7]+m[15]+m[19]).lower()], \n",
    "                                                                int(m[6]+m[14]+m[18])),\n",
    "                              'flagged': True, 'ts': ts\n",
    "                             })\n",
    "\n",
    "    else:\n",
    "        if list_full_events.find('span', {'class': 'flag_us'}) is None:\n",
    "            print(f'Skipping {ts}: no flags')\n",
    "            skipped_soups4.append((s,ts))\n",
    "            continue\n",
    "\n",
    "        for e in events:\n",
    "            if e.find('span', {'class': 'flag_us'}) is not None:\n",
    "                loc = e.find('span', {'itemprop': 'addressLocality'}).get_text()             \n",
    "                start_date = e.find('time', {'itemprop': 'startDate'}).attrs['datetime']\n",
    "                end_date = e.find('time', {'itemprop': 'endDate'})\n",
    "                if end_date is None:\n",
    "                    end_date = start_date\n",
    "                else:\n",
    "                    end_date = end_date.attrs['datetime']\n",
    "                start_date = datetime.date.fromisoformat(start_date)\n",
    "                end_date = datetime.date.fromisoformat(end_date)\n",
    "                description = e.find('span', {'itemprop': 'name'}).get_text()\n",
    "\n",
    "                dates.append({\n",
    "                    'location': location,\n",
    "                    'description': description,\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'flagged': True, 'ts': ts\n",
    "                })\n",
    "        \n",
    "print(f'Processed {len(soups) - len(skipped_soups4)}/{len(soups)} htmls.\\n So far {len(dates)} dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 20090219140144: no flags?\n",
      "Skipping 20090221180808: no flags?\n",
      "Skipping 20090222192240: no flags?\n",
      "Skipping 20090421070047: no flags?\n",
      "Skipping 20090820180634: no flags?\n",
      "Skipping 20100424005357: no events\n",
      "Skipping 20100429143621: no events\n",
      "Processed 261/268 htmls.\n",
      " So far 9407 dates.\n"
     ]
    }
   ],
   "source": [
    "skipped_soups5 = []\n",
    "for s, ts in skipped_soups4:\n",
    "    list_full_events = s.find(\"div\", {\"class\": \"list_full_events\"})\n",
    "    if list_full_events is None:\n",
    "        midcontainer = s.find(\"div\", {\"id\": \"midcontainer\"})\n",
    "        if midcontainer.find(\"div\", {\"class\": \"column_flag\"}) is None:\n",
    "            print(f'Skipping {ts}: no flags?')\n",
    "            skipped_soups5.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        events = midcontainer.find_all('div', {'class': 'rowoff'})\n",
    "        if not len(events):\n",
    "            print(f'Skipping {ts}: no rowoff div')\n",
    "            skipped_soups5.append((s,ts))\n",
    "            continue\n",
    "        \n",
    "            \n",
    "        if events[0].find('div', {'class': 'column_desc'}) is None or events[0].find_all('div', {'class': 'column_dateplace'}) is None:\n",
    "            print(f'Skipping {ts}: no column_desc or date_place div')\n",
    "            skipped_soups5.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        for e in events:\n",
    "            if 'cancelled' in e.get_text():\n",
    "                continue\n",
    "\n",
    "            # try to find flag\n",
    "            if 'us' not in e.find('img').attrs['alt'].lower():\n",
    "                continue\n",
    "\n",
    "            desc = e.find('div', {'class': 'column_desc'}).get_text()\n",
    "            loc, date = e.find_all('div', {'class': 'column_dateplace'})\n",
    "\n",
    "            m = re.findall('((\\d+)\\s+?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)|'\n",
    "                       '(\\d+)\\s+?[-–]\\s+?(\\d+) (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)|'\n",
    "                       '(\\d+)\\s+?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+?[-–]\\s+?'\n",
    "                       '(\\d+)\\s+?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))(\\s+?(20\\d\\d))?',\n",
    "                           date.get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' '))[0]\n",
    "\n",
    "            dates.append({'location': loc.get_text(),\n",
    "                          'description': desc,\n",
    "                          'start_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                              MONTHS_NAMES[(m[2]+m[5]+m[7]).lower()], int(m[1]+m[3]+m[6])),\n",
    "                          'end_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                             MONTHS_NAMES[(m[2]+m[5]+m[9]).lower()], int(m[1]+m[4]+m[8])),\n",
    "                          'flagged': True, 'ts': ts\n",
    "                         })\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        events = list_full_events.find_all(\"li\")\n",
    "        if not len(events):\n",
    "            print(f'Skipping {ts}: no events')\n",
    "            skipped_soups5.append((s,ts))\n",
    "            continue\n",
    "        \n",
    "        if e.find('span', {'class': 'flag_us'}) is not None:\n",
    "            loc = e.find('span', {'class': 'cel2'}).get_text()             \n",
    "            desc = e.find('span', {'class': 'cel4'}).get_text()             \n",
    "            date = e.find('span', {'class': 'cel3'}).get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' ')\n",
    "\n",
    "            m = re.findall(f'(({WEEKDAYS}\\s+?)?(\\d+)\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                           f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                           f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS})(\\s+(20\\d\\d))?', date)\n",
    "                \n",
    "            if len(m):\n",
    "                m = m[0]\n",
    "            else:\n",
    "                if 'TBA' in citydate.get_text():\n",
    "                    continue\n",
    "                else:\n",
    "                    raise NotImplementedError(\"matches\")\n",
    "\n",
    "            dates.append({'location': loc,\n",
    "                          'description': desc,\n",
    "                          'start_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                      MONTHS_NAMES[(m[7]+m[11]+m[19]).lower()], \n",
    "                                                      int(m[3]+m[10]+m[18])),\n",
    "                                  'end_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                            MONTHS_NAMES[(m[7]+m[15]+m[19]).lower()], \n",
    "                                                            int(m[6]+m[14]+m[18])),\n",
    "                          'flagged': True, 'ts': ts\n",
    "                         })\n",
    "        \n",
    "        \n",
    "print(f'Processed {len(soups) - len(skipped_soups5)}/{len(soups)} htmls.\\n So far {len(dates)} dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 268/268 htmls.\n",
      " So far 9544 dates.\n"
     ]
    }
   ],
   "source": [
    "skipped_soups6 = []\n",
    "for s, ts in skipped_soups5:\n",
    "    list_full_events = s.find(\"div\", {\"class\": \"list_full_events\"})\n",
    "    if list_full_events is None:\n",
    "        midcontainer = s.find(\"div\", {\"id\": \"midcontainer\"})\n",
    "        pridelistfull = s.find(\"div\", {\"class\": \"pridelistfull\"})\n",
    "        if pridelistfull is None:\n",
    "            print(f'Skipping {ts}: no pridelistfull')\n",
    "            skipped_soups6.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        events = midcontainer.find_all('div', {'class': 'row'})\n",
    "        if not len(events):\n",
    "            print(f'Skipping {ts}: no row div')\n",
    "            skipped_soups6.append((s,ts))\n",
    "            continue\n",
    "        \n",
    "        if events[0].find('div', {'class': 'cel1'}) is None:\n",
    "            print(f'Skipping {ts}: wrong format')\n",
    "            skipped_soups6.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        for e in events:\n",
    "            if 'cancelled' in e.get_text():\n",
    "                continue\n",
    "\n",
    "            # try to find flag\n",
    "            if 'us' not in e.find('img').attrs['alt'].lower():\n",
    "                continue\n",
    "            loc = e.find('div', {'class': 'cel2'}).get_text()             \n",
    "            desc = e.find('div', {'class': 'cel4'}).get_text()             \n",
    "            date = e.find('div', {'class': 'cel3'}).get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' ')\n",
    "\n",
    "            m = re.findall(f'(({WEEKDAYS}\\s+?)?(\\d+)\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                           f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                           f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS})(\\s+(20\\d\\d))?', date)\n",
    "                \n",
    "            if len(m):\n",
    "                m = m[0]\n",
    "            else:\n",
    "                if 'TBA' in citydate.get_text():\n",
    "                    continue\n",
    "                else:\n",
    "                    raise NotImplementedError(\"matches\")\n",
    "\n",
    "            dates.append({'location': loc,\n",
    "                          'description': desc,\n",
    "                          'start_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                      MONTHS_NAMES[(m[7]+m[11]+m[19]).lower()], \n",
    "                                                      int(m[3]+m[10]+m[18])),\n",
    "                          'end_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                            MONTHS_NAMES[(m[7]+m[15]+m[19]).lower()], \n",
    "                                                            int(m[6]+m[14]+m[18])),\n",
    "                          'flagged': True, 'ts': ts\n",
    "                         })\n",
    "        \n",
    "    else:\n",
    "        events = list_full_events.find_all(\"a\")\n",
    "        if not len(events):\n",
    "            print(f'Skipping {ts}: no events again')\n",
    "            skipped_soups6.append((s,ts))\n",
    "            continue\n",
    "            \n",
    "        if list_full_events.find('span', {'class': 'flag_us'}) is None:\n",
    "            print(f'Skipping {ts}: no flags again')\n",
    "            skipped_soups6.append((s,ts))\n",
    "            continue\n",
    "        \n",
    "        for e in events:\n",
    "            if e.find('span', {'class': 'flag_us'}) is not None:\n",
    "                loc = e.find('span', {'class': 'cel2'}).get_text()             \n",
    "                desc = e.find('span', {'class': 'cel4'}).get_text()             \n",
    "                date = e.find('span', {'class': 'cel3'}).get_text().replace(u'\\xa0', u' ').replace('\\u2009', u' ')\n",
    "\n",
    "                m = re.findall(f'(({WEEKDAYS}\\s+?)?(\\d+)\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                               f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}\\s+?[-–]\\s+?({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS}|'\n",
    "                               f'({WEEKDAYS}\\s+?)?(\\d+)\\s+?{MONTHS})(\\s+(20\\d\\d))?', date)\n",
    "                \n",
    "                if len(m):\n",
    "                    m = m[0]\n",
    "                else:\n",
    "                    if 'TBA' in citydate.get_text():\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"matches\")\n",
    "\n",
    "                dates.append({'location': loc,\n",
    "                              'description': desc,\n",
    "                              'start_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                          MONTHS_NAMES[(m[7]+m[11]+m[19]).lower()], \n",
    "                                                          int(m[3]+m[10]+m[18])),\n",
    "                              'end_date': datetime.date(int(m[-1] if len(m[-1]) else ts[:4]), \n",
    "                                                                MONTHS_NAMES[(m[7]+m[15]+m[19]).lower()], \n",
    "                                                                int(m[6]+m[14]+m[18])),\n",
    "                              'flagged': True, 'ts': ts\n",
    "                             })\n",
    "        \n",
    "        \n",
    "print(f'Processed {len(soups) - len(skipped_soups6)}/{len(soups)} htmls.\\n So far {len(dates)} dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_dates = sorted(dates, key=lambda d: d['start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_sorted_dates = {} \n",
    "\n",
    "for d in list(sorted_dates):\n",
    "    filt_sorted_dates[(d['location'], d['start_date'], d['end_date'])] = {'desc': d['description'],\n",
    "                                                                          'flagged': d['flagged'],\n",
    "                                                                          'ts': d['ts']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( filt_sorted_dates.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([k for k in filt_sorted_dates.keys() if filt_sorted_dates[k]['flagged']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('unfiltered.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['location', 'start_date', 'end_date', 'description', 'USA for sure', 'timestamp']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for d in sorted_dates:\n",
    "        writer.writerow({'location': d['location'],\n",
    "                         'start_date': d['start_date'],\n",
    "                         'end_date': d['end_date'],\n",
    "                         'description': d['description'],\n",
    "                         'USA for sure': d['flagged'],\n",
    "                         'timestamp': d['ts'],\n",
    "                        })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('filtered.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['location', 'start_date', 'end_date', 'description', 'USA for sure', 'timestamp']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for k in filt_sorted_dates:\n",
    "        writer.writerow({'location': k[0],\n",
    "                         'start_date': k[1],\n",
    "                         'end_date': k[2],\n",
    "                         'description': filt_sorted_dates[k]['desc'],\n",
    "                         'USA for sure': filt_sorted_dates[k]['flagged'],\n",
    "                         'timestamp': filt_sorted_dates[k]['ts'],\n",
    "                        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7f33f04a4cbdb70ba90e03e6c9c0feecc887078ae9be8d50c6ef5632ee6aa58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
